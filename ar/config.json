{
  "noInstanceSelected": "لم يتم اختيار أي نموذج",
  "resetToDefault": "إعادة تعيين",
  "showAdvancedSettings": "إظهار الإعدادات المتقدمة",
  "showAll": "إظهار الكل",
  "basicSettings": "إعدادات أساسية",
  "configSubtitle": "تحميل أو حفظ الإعدادات المسبقة والتجربة مع إعدادات النموذج",
  "inferenceParameters/title": "إعدادات التنبؤ",
  "inferenceParameters/info": "تجربة مع الإعدادات التي تؤثر على التنبؤ.",
  "generalParameters/title": "عام",
  "samplingParameters/title": "العينة",
  "basicTab": "أساسي",
  "advancedTab": "متقدم",
  "loadInstanceFirst": "قم بتحميل نموذج لرؤية الإعدادات القابلة للتخصيص",
  "noListedConfigs": "لا توجد إعدادات قابلة للتخصيص",
  "generationParameters/info": "تجربة مع الإعدادات الأساسية التي تؤثر على توليد النص.",
  "loadParameters/title": "تحميل الإعدادات",
  "loadParameters/description": "يتطلب تعديل هذه الإعدادات إعادة تحميل النموذج",
  "loadParameters/reload": "إعادة تحميل لتطبيق التعديلات على إعدادات التحميل",
  "discardChanges": "إلغاء التعديلات",
  "llm.prediction.systemPrompt/title": "إرشادات للذكاء الاصطناعي",
  "llm.prediction.systemPrompt/description": "استخدم هذا الحقل لتوفير تعليمات أساسية للنموذج، مثل مجموعة من القواعد أو القيود أو المتطلبات العامة. يُسمى هذا الحقل أيضًا « دعوة النظام ».",
  "llm.prediction.temperature/title": "درجة الحرارة",
  "llm.prediction.temperature/info": "وفقًا لوثائق llama.cpp: « القيمة الافتراضية هي <{{dynamicValue}}>، التي توفر توازنًا بين العشوائية والحتمية. في أقصى الحدود، درجة الحرارة 0 ستختار دائمًا الرمز الأكثر احتمالًا، مما يؤدي إلى إخراجات متماثلة في كل مرة. »",
  "llm.prediction.topKSampling/title": "عينة أعلى K",
  "llm.prediction.topKSampling/info": "وفقًا لوثائق llama.cpp:\n\nعينة Top-k هي طريقة لتوليد النص تختار الرمز التالي فقط من بين k الرموز الأكثر احتمالاً التي يتنبأ بها النموذج.\n\nيساعد ذلك في تقليل خطر توليد رموز ذات احتمالية منخفضة أو غير معقولة، ولكن قد يحد أيضًا من تنوع المخرجات.\n\nالقيمة الافتراضية هي <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "خيوط المعالج",
  "llm.prediction.llama.cpuThreads/info": "عدد الخيوط التي سيتم استخدامها أثناء الحساب. زيادة عدد الخيوط لا يرتبط دائمًا بتحسين الأداء. القيمة الافتراضية هي <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "تحديد طول الاستجابة",
  "llm.prediction.maxPredictedTokens/info": "تحكم في الطول الأقصى لاستجابة الشات بوت. قم بتمكينه لتحديد حد لطول الاستجابة أو قم بإيقافه ليقرر الشات بوت متى يتوقف.",
  "llm.prediction.maxPredictedTokens/inputLabel": "أقصى طول للاستجابة (رموز)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "حوالي {{maxWords}} كلمات",
  "llm.prediction.repeatPenalty/title": "عقوبة التكرار",
  "llm.prediction.repeatPenalty/info": "وفقًا لوثائق llama.cpp: « يساعد في منع النموذج من توليد نص مكرر أو ممل.\n\nالقيمة الأعلى (مثل 1.5) ستعاقب التكرار بقوة أكبر، في حين أن القيمة الأقل (مثل 0.9) ستكون أكثر تساهلاً. » • القيمة الافتراضية هي <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "عينة Min P",
  "llm.prediction.minPSampling/info": "وفقًا لوثائق llama.cpp:\n\nالاحتمالية الدنيا لكي يتم اعتبار الرمز، نسبة إلى الاحتمالية الأعلى للرمز. يجب أن تكون ضمن [0, 1].\n\n• القيمة الافتراضية هي <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "عينة Top P",
  "llm.prediction.topPSampling/info": "وفقًا لوثائق llama.cpp:\n\nعينة Top-p، والمعروفة أيضًا بعينة nucleus، هي طريقة أخرى لتوليد النص تختار الرمز التالي من بين مجموعة فرعية من الرموز التي تكون الاحتمالات التراكمية لها على الأقل p.\n\nتوفر هذه الطريقة توازنًا بين التنوع والجودة من خلال أخذ في الاعتبار كل من احتمالات الرموز وعدد الرموز التي سيتم أخذ العينات منها.\n\nالقيمة الأعلى لـ top-p (مثل 0.95) ستؤدي إلى نص أكثر تنوعًا، بينما ستؤدي القيمة الأقل (مثل 0.5) إلى نص أكثر تركيزًا ومحافظة. يجب أن تكون ضمن (0، 1].\n\n• القيمة الافتراضية هي <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "سلاسل التوقف",
  "llm.prediction.stopStrings/info": "سلاسل معينة، عند مواجهتها، ستوقف النموذج عن توليد المزيد من الرموز",
  "llm.prediction.stopStrings/placeholder": "أدخل سلسلة واضغط على ⏎",
  "llm.prediction.contextOverflowPolicy/title": "تدفق المحادثة",
  "llm.prediction.contextOverflowPolicy/info": "قرّر ماذا تفعل عندما تتجاوز المحادثة حجم ذاكرة النموذج ('السياق')",
  "customInputs.contextOverflowPolicy.stopAtLimit": "التوقف عند الحد",
  "customInputs.contextOverflowPolicy.stopAtLimitSub": "التوقف عن التوليد عندما تمتلئ ذاكرة النموذج",
  "customInputs.contextOverflowPolicy.truncateMiddle": "قطع الجزء الأوسط",
  "customInputs.contextOverflowPolicy.truncateMiddleSub": "إزالة الرسائل من وسط المحادثة لإفساح المجال للرسائل الأحدث. سيظل النموذج يتذكر بداية المحادثة",
  "customInputs.contextOverflowPolicy.rollingWindow": "نافذة متدحرجة",
  "customInputs.contextOverflowPolicy.rollingWindowSub": "سيحصل النموذج دائمًا على الرسائل الأحدث ولكن قد ينسى بداية المحادثة",
  "llm.prediction.llama.frequencyPenalty/title": "عقوبة التكرار",
  "llm.prediction.llama.presencePenalty/title": "عقوبة الوجود",
  "llm.prediction.llama.tailFreeSampling/title": "عينة بدون ذيل",
  "llm.prediction.llama.locallyTypicalSampling/title": "عينة نموذجية محلية",
  "llm.prediction.onnx.topKSampling/title": "عينة Top K",
  "llm.prediction.onnx.topKSampling/info": "وفقًا لوثائق ONNX:\n\nعدد الرموز التي تبقى لها أعلى احتمالية للاحتفاظ بها للترشيح top-k\n\n• هذا الفلتر غير مفعل افتراضيًا",
  "llm.prediction.onnx.repeatPenalty/title": "عقوبة التكرار",
  "llm.prediction.onnx.repeatPenalty/info": "القيمة الأعلى تمنع النموذج من التكرار",
  "llm.prediction.onnx.topPSampling/title": "عينة Top P",
  "llm.prediction.onnx.topPSampling/info": "وفقًا لوثائق ONNX:\n\nيتم الاحتفاظ فقط بالرموز الأكثر احتمالًا التي تكون الاحتمالات التراكمية لها على الأقل TopP أو أكثر لتوليدها\n\n• هذا الفلتر غير مفعل افتراضيًا",
  "llm.prediction.seed/title": "البذرة",
  "llm.prediction.structured/title": "الإخراج الهيكلي",
  "llm.prediction.structured/info": "الإخراج الهيكلي",
  "llm.load.contextLength/title": "طول السياق",
  "llm.load.contextLength/info": "يحدد الحد الأقصى لعدد الرموز التي يمكن أن يأخذها النموذج في الاعتبار في وقت واحد، مما يؤثر على كمية السياق التي يحتفظ بها أثناء المعالجة",
  "llm.load.seed/title": "البذرة",
  "llm.load.seed/info": "البذرة العشوائية: تحدد البذرة لتوليد الأرقام العشوائية لضمان نتائج قابلة للتكرار",
  "llm.load.llama.evalBatchSize/title": "حجم الدفعة للتقييم",
  "llm.load.llama.evalBatchSize/info": "يحدد عدد الأمثلة التي تتم معالجتها معًا في دفعة أثناء التقييم، مما يؤثر على السرعة واستخدام الذاكرة",
  "llm.load.llama.ropeFrequencyBase/title": "قاعدة تردد RoPE",
  "llm.load.llama.ropeFrequencyBase/info": "[متقدم] يعدل تردد الأساس للتشفير الموضعي الدوار، مما يؤثر على كيفية دمج المعلومات الموضعية",
  "llm.load.llama.ropeFrequencyScale/title": "مقياس تردد RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[متقدم] يعدل مقياس التردد للتشفير الموضعي الدوار للتحكم في دقة التشفير الموضعي",
  "llm.load.llama.acceleration.offloadRatio/title": "تحميل GPU",
  "llm.load.llama.acceleration.offloadRatio/info": "حدد نسبة الحساب التي سيتم تحميلها على الـGPU. قم بإيقافه لتعطيل تحميل الـGPU أو قم باستخدام الوضع التلقائي ليقرر النموذج.",
  "llm.load.llama.flashAttention/title": "الانتباه السريع",
  "llm.load.llama.flashAttention/info": "يسرع آليات الانتباه لمعالجة أسرع وأكثر كفاءة",
  "llm.load.llama.keepModelInMemory/title": "الحفاظ على النموذج في الذاكرة",
  "llm.load.llama.keepModelInMemory/info": "يمنع النموذج من أن يتم تبادله إلى القرص، مما يضمن وصولًا أسرع على حساب استخدام الذاكرة العشوائية",
  "llm.load.llama.useFp16ForKVCache/title": "استخدام FP16 لذاكرة KV",
  "llm.load.llama.useFp16ForKVCache/info": "يقلل من استخدام الذاكرة عن طريق تخزين ذاكرة التخزين المؤقت بدقة نصفية (FP16)",
  "llm.load.llama.tryMmap/title": "تجربة mmap()",
  "llm.load.llama.tryMmap/info": "قم بتحميل ملفات النموذج مباشرة من القرص إلى الذاكرة"
}
