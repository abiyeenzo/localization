{
  "noInstanceSelected": "لم يتم اختيار أي نموذج",
  "resetToDefault": "إعادة تعيين",
  "showAdvancedSettings": "عرض الإعدادات المتقدمة",
  "showAll": "عرض الكل",
  "basicSettings": "الإعدادات الأساسية",
  "configSubtitle": "تحميل أو حفظ الإعدادات المبدئية والتجربة مع إعدادات النموذج",
  "inferenceParameters/title": "إعدادات التنبؤ",
  "inferenceParameters/info": "جرب الإعدادات التي تؤثر على التنبؤ.",
  "generalParameters/title": "عام",
  "samplingParameters/title": "العينة",
  "basicTab": "أساسي",
  "advancedTab": "متقدم",
  "loadInstanceFirst": "قم بتحميل نموذج لمعاينة الإعدادات القابلة للتعديل",
  "noListedConfigs": "لا توجد إعدادات قابلة للتعديل",
  "generationParameters/info": "جرب الإعدادات الأساسية التي تؤثر على توليد النص.",
  "loadParameters/title": "تحميل الإعدادات",
  "loadParameters/description": "يتطلب تعديل هذه الإعدادات إعادة تحميل النموذج",
  "loadParameters/reload": "إعادة التحميل لتطبيق التعديلات على إعدادات التحميل",
  "discardChanges": "إلغاء التعديلات",
  "llm.prediction.systemPrompt/title": "إرشادات للذكاء الاصطناعي",
  "llm.prediction.systemPrompt/description": "استخدم هذا الحقل لتوفير تعليمات أساسية للنموذج مثل مجموعة من القواعد أو القيود أو المتطلبات العامة. يُسمى هذا الحقل أحيانًا «دعوة النظام».",
  "llm.prediction.temperature/title": "درجة الحرارة",
  "llm.prediction.temperature/info": "وفقًا لوثائق المساعدة لـ llama.cpp : « القيمة الافتراضية هي <{{dynamicValue}}>، والتي توفر توازنًا بين العشوائية والحتمية. عند أقصى حد، فإن درجة الحرارة 0 ستختار دائمًا الرموز الأكثر احتمالاً، مما يؤدي إلى مخرجات متماثلة في كل مرة »",
  "llm.prediction.topKSampling/title": "العينة العلوية K",
  "llm.prediction.topKSampling/info": "وفقًا لوثائق المساعدة لـ llama.cpp :\n\nالعينة العلوية K هي طريقة لتوليد النص تختار الرمز التالي فقط من بين أعلى k رموز محتملة يتنبأ بها النموذج.\n\nيساعد ذلك في تقليل خطر توليد رموز ذات احتمالية منخفضة أو غير منطقية، ولكن قد يحد أيضًا من تنوع المخرجات.\n\nالقيمة الافتراضية هي <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "خيوط المعالج المركزي",
  "llm.prediction.llama.cpuThreads/info": "عدد الخيوط المستخدمة أثناء الحساب. زيادة عدد الخيوط لا يرتبط دائمًا بتحسين الأداء. القيمة الافتراضية هي <{{dynamicValue}}>. ",
  "llm.prediction.maxPredictedTokens/title": "تحديد طول الإجابة",
  "llm.prediction.maxPredictedTokens/info": "تحكم في الحد الأقصى لطول إجابة روبوت الدردشة. قم بتفعيل لتحديد حد لطول الإجابة، أو قم بإيقافه ليقرر روبوت الدردشة متى يتوقف.",
  "llm.prediction.maxPredictedTokens/inputLabel": "الطول الأقصى للإجابة (الرموز)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "حوالي {{maxWords}} كلمات",
  "llm.prediction.repeatPenalty/title": "عقوبة التكرار",
  "llm.prediction.repeatPenalty/info": "وفقًا لوثائق المساعدة لـ llama.cpp : « يساعد في منع النموذج من توليد نص مكرر أو رتيب.\n\nالقيمة الأعلى (مثل 1.5) ستعاقب التكرار بشكل أقوى، بينما القيمة الأقل (مثل 0.9) ستكون أكثر تساهلاً. » • القيمة الافتراضية هي <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "العينة Min P",
  "llm.prediction.minPSampling/info": "وفقًا لوثائق المساعدة لـ llama.cpp :\n\nالاحتمالية الدنيا التي يجب أن يكون عليها الرمز ليتم النظر فيه، بالنسبة لأعلى احتمال للرمز. يجب أن تكون في [0، 1].\n\n• القيمة الافتراضية هي <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "العينة العلوية P",
  "llm.prediction.topPSampling/info": "وفقًا لوثائق المساعدة لـ llama.cpp :\n\nالعينة العلوية p، والمعروفة أيضًا بالعينة المركزية، هي طريقة أخرى لتوليد النص تختار الرمز التالي من بين مجموعة من الرموز التي تكون احتمالاتها التراكمية لا تقل عن p.\n\nتوفر هذه الطريقة توازنًا بين التنوع والجودة من خلال النظر في احتمالات الرموز وعدد الرموز التي سيتم اختيارها.\n\nالقيمة الأعلى للـ top-p (مثل 0.95) ستؤدي إلى نص أكثر تنوعًا، بينما القيمة الأقل (مثل 0.5) ستولد نصًا أكثر تركيزًا ومحافظة. يجب أن تكون في (0، 1].\n\n• القيمة الافتراضية هي <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "سلاسل التوقف",
  "llm.prediction.stopStrings/info": "سلاسل محددة عند مواجهتها، ستوقف النموذج من توليد المزيد من الرموز",
  "llm.prediction.stopStrings/placeholder": "أدخل السلسلة واضغط ⏎",
  "llm.prediction.contextOverflowPolicy/title": "تجاوز السياق",
  "llm.prediction.contextOverflowPolicy/info": "قرر ما يجب فعله عندما يتجاوز المحادثة حجم ذاكرة العمل للنموذج ('السياق')",
  "customInputs.contextOverflowPolicy.stopAtLimit": "التوقف عند الحد",
  "customInputs.contextOverflowPolicy.stopAtLimitSub": "توقف عن التوليد بمجرد امتلاء ذاكرة النموذج",
  "customInputs.contextOverflowPolicy.truncateMiddle": "اقتطاع المنتصف",
  "customInputs.contextOverflowPolicy.truncateMiddleSub": "احذف الرسائل من وسط المحادثة لإفساح المجال للأحدث. سيتذكر النموذج دائمًا بداية المحادثة",
  "customInputs.contextOverflowPolicy.rollingWindow": "نافذة منزلقة",
  "customInputs.contextOverflowPolicy.rollingWindowSub": "سيحصل النموذج دائمًا على الرسائل القليلة الأحدث ولكن قد ينسى بداية المحادثة",
  "llm.prediction.llama.frequencyPenalty/title": "عقوبة التكرار",
  "llm.prediction.llama.presencePenalty/title": "عقوبة الوجود",
  "llm.prediction.llama.tailFreeSampling/title": "العينة بدون ذيل",
  "llm.prediction.llama.locallyTypicalSampling/title": "العينة النمطية المحلية",
  "llm.prediction.onnx.topKSampling/title": "العينة العلوية K",
  "llm.prediction.onnx.topKSampling/info": "وفقًا لوثائق ONNX :\n\nعدد الرموز ذات الاحتمالات الأعلى التي يجب الاحتفاظ بها للتصفية العلوية K\n\n• هذا الفلتر غير مفعل افتراضيًا",
  "llm.prediction.onnx.repeatPenalty/title": "عقوبة التكرار",
  "llm.prediction.onnx.repeatPenalty/info": "القيمة الأعلى تمنع النموذج من التكرار",
  "llm.prediction.onnx.topPSampling/title": "العينة العلوية P",
  "llm.prediction.onnx.topPSampling/info": "وفقًا لوثائق ONNX :\n\nفقط الرموز الأكثر احتمالًا التي تجمع احتمالاتها إلى TopP أو أكثر يتم الاحتفاظ بها للتوليد\n\n• هذا الفلتر غير مفعل افتراضيًا",
  "llm.prediction.seed/title": "البذرة",
  "llm.prediction.structured/title": "الإخراج المنظم",
  "llm.prediction.structured/info": "الإخراج المنظم",
  "llm.load.contextLength/title": "طول السياق",
  "llm.load.contextLength/info": "تحديد الحد الأقصى للعدد الرمزي الذي يمكن للنموذج أخذه في الاعتبار في وقت واحد، مما يؤثر على كمية السياق الذي يحتفظ به أثناء المعالجة",
  "llm.load.seed/title": "البذرة",
  "llm.load.seed/info": "البذرة العشوائية: تحدد البذرة لتوليد الأرقام العشوائية لضمان النتائج القابلة لإعادة التكرار",
  "llm.load.llama.evalBatchSize/title": "حجم الدفعة للتقييم",
  "llm.load.llama.evalBatchSize/info": "يحدد عدد الأمثلة التي يتم معالجتها معًا في دفعة أثناء التقييم، مما يؤثر على السرعة واستخدام الذاكرة",
  "llm.load.llama.ropeFrequencyBase/title": "أساس التردد RoPE",
  "llm.load.llama.ropeFrequencyBase/info": "[متقدم] ضبط الأساس للترددات من أجل الترميز الموضعي المتداول، مما يؤثر على كيفية دمج المعلومات الموضعية",
  "llm.load.llama.ropeFrequencyScale/title": "مقياس التردد RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[متقدم] تعديل مقياس التردد للترميز الموضعي المتداول للتحكم في دقة الترميز الموضعي",
  "llm.load.llama.acceleration.offloadRatio/title": "نقل الحمل GPU",
  "llm.load.llama.acceleration.offloadRatio/info": "حدد نسبة الحساب التي يتم تفريغها إلى وحدة المعالجة الرسومية. قم بإيقافه لتعطيل التفريغ GPU، أو اضغط تلقائيًا لترك النموذج يقرر.",
  "llm.load.llama.flashAttention/title": "الاهتمام الفلاش",
  "llm.load.llama.flashAttention/info": "يُسرع آليات الانتباه للحصول على معالجة أسرع وأكثر كفاءة",
  "llm.load.llama.keepModelInMemory/title": "الحفاظ على النموذج في الذاكرة",
  "llm.load.llama.keepModelInMemory/info": "يمنع النموذج من أن يتم استبداله على القرص، مما يضمن الوصول الأسرع على حساب زيادة استخدام الذاكرة العشوائية",
  "llm.load.llama.useFp16ForKVCache/title": "استخدام FP16 للذاكرة المؤقتة KV",
  "llm.load.llama.useFp16ForKVCache/info": "يقلل من استخدام الذاكرة عن طريق تخزين الذاكرة المؤقتة بدقة نصفية (FP16)",
  "llm.load.llama.tryMmap/title": "محاولة mmap()",
  "llm.load.llama.tryMmap/info": "قم بتحميل ملفات النموذج مباشرة من القرص إلى الذاكرة"
}
